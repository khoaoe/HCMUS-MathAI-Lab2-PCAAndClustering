{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnB_pjlKAe_D"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "LbcUI5J8Ae_J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "def confusion_matrix(y_true, y_pred):\n",
        "    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "    return np.array([[tn, fp], [fn, tp]])\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    return np.mean(np.asarray(y_true) == np.asarray(y_pred))\n",
        "\n",
        "def precision_score(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tp, fp = cm[1,1], cm[0,1]\n",
        "    return tp / (tp + fp) if (tp + fp) else 0\n",
        "\n",
        "def recall_score(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tp, fn = cm[1,1], cm[1,0]\n",
        "    return tp / (tp + fn) if (tp + fn) else 0\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    p = precision_score(y_true, y_pred)\n",
        "    r = recall_score(y_true, y_pred)\n",
        "    return 2 * p * r / (p + r) if (p + r) else 0\n",
        "\n",
        "def silhouette_score(X, labels):\n",
        "    X = np.asarray(X)\n",
        "    labels = np.asarray(labels)\n",
        "    n = X.shape[0]\n",
        "    uniq = np.unique(labels)\n",
        "\n",
        "    D = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        D[i, i+1:] = np.sum((X[i] - X[i+1:])**2, axis=1)\n",
        "    D = D + D.T  # Make symmetric\n",
        "    \n",
        "    sil = np.zeros(n)\n",
        "    for label in uniq:\n",
        "        mask = labels == label\n",
        "        points_in_cluster = np.where(mask)[0]\n",
        "        \n",
        "        if len(points_in_cluster) <= 1:\n",
        "            continue\n",
        "            \n",
        "        a_values = np.zeros(len(points_in_cluster))\n",
        "        for i, idx in enumerate(points_in_cluster):\n",
        "            a_values[i] = np.mean(D[idx, points_in_cluster[points_in_cluster != idx]])\n",
        "        \n",
        "        b_values = np.ones(len(points_in_cluster)) * np.inf\n",
        "        for other_label in uniq:\n",
        "            if other_label == label:\n",
        "                continue\n",
        "                \n",
        "            other_mask = labels == other_label\n",
        "            other_points = np.where(other_mask)[0]\n",
        "            \n",
        "            if len(other_points) == 0:\n",
        "                continue\n",
        "                \n",
        "            for i, idx in enumerate(points_in_cluster):\n",
        "                mean_dist = np.mean(D[idx, other_points])\n",
        "                b_values[i] = min(b_values[i], mean_dist)\n",
        "        \n",
        "        sil_values = (b_values - a_values) / np.maximum(a_values, b_values)\n",
        "        sil[points_in_cluster] = sil_values\n",
        "        \n",
        "    return np.mean(sil)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4NvjZ0qAe_L"
      },
      "source": [
        "# class MyPCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "-1tFgqXFAe_L"
      },
      "outputs": [],
      "source": [
        "class MyPCA:\n",
        "    def __init__(self, n_components=None):\n",
        "        self.n_components = n_components\n",
        "\n",
        "    def fit(self, X):\n",
        "        # 1) Tâm dữ liệu\n",
        "        self.mean_ = np.mean(X, axis=0)\n",
        "        Xc = X - self.mean_\n",
        "        # 2) Ma trận hiệp phương sai\n",
        "        n_samples = X.shape[0]\n",
        "        if X.shape[0] > 10000:\n",
        "            # phong truong hop du lieu lon\n",
        "            chunk_size = 1000\n",
        "            C = np.zeros((X.shape[1], X.shape[1]))\n",
        "            for i in range(0, n_samples, chunk_size):\n",
        "                end = min(i + chunk_size, n_samples)\n",
        "                chunk = Xc[i:end]\n",
        "                C += np.dot(chunk.T, chunk)\n",
        "            C /= (n_samples - 1)\n",
        "        else:\n",
        "            C = np.dot(Xc.T, Xc) / (n_samples - 1)\n",
        "        # 3) Tính trị riêng và vector riêng\n",
        "        eigvals, eigvecs = np.linalg.eigh(C)\n",
        "        # 4) Sắp xếp giảm dần\n",
        "        idx = np.argsort(eigvals)[::-1]\n",
        "        eigvals = eigvals[idx]\n",
        "        eigvecs = eigvecs[:, idx]\n",
        "        # 5) Chọn số thành phần\n",
        "        if self.n_components is not None:\n",
        "            eigvals = eigvals[:self.n_components]\n",
        "            eigvecs = eigvecs[:, :self.n_components]\n",
        "        # 6) Lưu kết quả\n",
        "        self.components_ = eigvecs.T\n",
        "        self.explained_variance_ = eigvals\n",
        "        self.total_var_ = np.sum(np.linalg.eigvalsh(C))  # Total variance\n",
        "        self.explained_variance_ratio_ = eigvals / self.total_var_\n",
        "        self.cumulative_explained_variance_ = np.cumsum(self.explained_variance_ratio_)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        Xc = X - self.mean_\n",
        "\n",
        "        return np.dot(Xc, self.components_.T)\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        return self.fit(X).transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rs97ryAAe_N"
      },
      "source": [
        "# KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "ePr7KcUqAe_O"
      },
      "outputs": [],
      "source": [
        "def kmeans(X, k=2, max_iters=100, tol=1e-6, n_init=10, init_method='kmeans++', random_state=None, early_stopping_iter=5, batch_size=None):\n",
        "    n_samples, n_features = X.shape\n",
        "    rng = np.random.RandomState(random_state) if random_state is not None else np.random\n",
        "    \n",
        "    # Optimize batch size\n",
        "    if batch_size is None:\n",
        "        batch_size = min(1000, n_samples)\n",
        "    \n",
        "    best_inertia = float('inf')\n",
        "    best_labels = None\n",
        "    best_centers = None\n",
        "    \n",
        "    # Pre-allocate distance matrix\n",
        "    distances = np.zeros((min(batch_size, n_samples), k))\n",
        "    \n",
        "    for init_attempt in range(n_init):\n",
        "        # Initialize centers\n",
        "        if init_method == 'kmeans++':\n",
        "            centers = np.zeros((k, n_features))\n",
        "            # First center randomly\n",
        "            idx = rng.choice(n_samples)\n",
        "            centers[0] = X[idx].copy()\n",
        "            \n",
        "            # Choose remaining centers\n",
        "            for i in range(1, k):\n",
        "                # Calculate minimum distances to existing centers\n",
        "                min_dists = np.zeros(n_samples)\n",
        "                for j in range(n_samples):\n",
        "                    min_dist = float('inf')\n",
        "                    for c in range(i):\n",
        "                        dist = np.sum((X[j] - centers[c])**2)\n",
        "                        if dist < min_dist:\n",
        "                            min_dist = dist\n",
        "                    min_dists[j] = min_dist\n",
        "                \n",
        "                # Choose next center with probability proportional to distance squared\n",
        "                sum_dists = np.sum(min_dists)\n",
        "                if sum_dists > 0:\n",
        "                    probs = min_dists / sum_dists\n",
        "                    idx = rng.choice(n_samples, p=probs)\n",
        "                else:\n",
        "                    idx = rng.choice(n_samples)\n",
        "                \n",
        "                centers[i] = X[idx].copy()\n",
        "        else:\n",
        "            # Random initialization\n",
        "            indices = rng.choice(n_samples, size=k, replace=False)\n",
        "            centers = X[indices].copy()\n",
        "        \n",
        "        # EM optimization\n",
        "        labels = np.zeros(n_samples, dtype=int)\n",
        "        prev_inertias = []\n",
        "        \n",
        "        for iteration in range(max_iters):\n",
        "            # Process in batches for large datasets\n",
        "            changed = False\n",
        "            inertia = 0\n",
        "            \n",
        "            for start in range(0, n_samples, batch_size):\n",
        "                end = min(start + batch_size, n_samples)\n",
        "                batch_indices = np.arange(start, end)\n",
        "                X_batch = X[batch_indices]\n",
        "                batch_size_actual = end - start\n",
        "                \n",
        "                # Calculate distances efficiently\n",
        "                for i in range(k):\n",
        "                    distances[:batch_size_actual, i] = np.sum((X_batch - centers[i])**2, axis=1)\n",
        "                \n",
        "                # Update labels\n",
        "                new_labels = np.argmin(distances[:batch_size_actual], axis=1)\n",
        "                if not np.array_equal(labels[batch_indices], new_labels):\n",
        "                    changed = True\n",
        "                    labels[batch_indices] = new_labels\n",
        "                \n",
        "                # Calculate partial inertia\n",
        "                for i in range(k):\n",
        "                    cluster_points = X_batch[new_labels == i]\n",
        "                    if len(cluster_points) > 0:\n",
        "                        inertia += np.sum((cluster_points - centers[i])**2)\n",
        "            \n",
        "            # Check for empty clusters and handle them\n",
        "            cluster_sizes = np.bincount(labels, minlength=k)\n",
        "            empty_clusters = np.where(cluster_sizes == 0)[0]\n",
        "            \n",
        "            if len(empty_clusters) > 0:\n",
        "                for empty_idx in empty_clusters:\n",
        "                    # Find largest cluster\n",
        "                    largest_idx = np.argmax(cluster_sizes)\n",
        "                    if cluster_sizes[largest_idx] <= 1:\n",
        "                        continue\n",
        "                    \n",
        "                    # Find point in largest cluster farthest from its center\n",
        "                    mask = labels == largest_idx\n",
        "                    points = X[mask]\n",
        "                    center = centers[largest_idx]\n",
        "                    distances_to_center = np.sum((points - center)**2, axis=1)\n",
        "                    farthest_idx = np.argmax(distances_to_center)\n",
        "                    \n",
        "                    # Get original index and reassign\n",
        "                    orig_idx = np.where(mask)[0][farthest_idx]\n",
        "                    labels[orig_idx] = empty_idx\n",
        "                    \n",
        "                    # Update cluster sizes\n",
        "                    cluster_sizes[empty_idx] += 1\n",
        "                    cluster_sizes[largest_idx] -= 1\n",
        "            \n",
        "            # Update centers\n",
        "            new_centers = np.zeros_like(centers)\n",
        "            for i in range(k):\n",
        "                mask = labels == i\n",
        "                if np.any(mask):\n",
        "                    new_centers[i] = np.mean(X[mask], axis=0)\n",
        "                else:\n",
        "                    new_centers[i] = centers[i]\n",
        "            \n",
        "            # Check convergence\n",
        "            center_shift = np.sum((centers - new_centers)**2)\n",
        "            centers = new_centers\n",
        "            \n",
        "            # Early stopping\n",
        "            prev_inertias.append(inertia)\n",
        "            if len(prev_inertias) > early_stopping_iter:\n",
        "                recent_improvements = np.diff(prev_inertias[-early_stopping_iter:])\n",
        "                if np.all(np.abs(recent_improvements) < tol * inertia) or not changed:\n",
        "                    break\n",
        "            \n",
        "            if center_shift < tol:\n",
        "                break\n",
        "        \n",
        "        # Keep best result\n",
        "        if inertia < best_inertia:\n",
        "            best_inertia = inertia\n",
        "            best_labels = labels.copy()\n",
        "            best_centers = centers.copy()\n",
        "    \n",
        "    return best_labels, best_centers, best_inertia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agglomerative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "def agglomerative_mst(X, k):\n",
        "    n = X.shape[0]\n",
        "    \n",
        "    # Use more memory-efficient distance calculation\n",
        "    D = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            D[i, j] = np.sum((X[i] - X[j])**2)\n",
        "            D[j, i] = D[i, j]  # Symmetric\n",
        "    \n",
        "    # Optimized Prim's algorithm\n",
        "    in_mst = np.zeros(n, bool)\n",
        "    in_mst[0] = True\n",
        "    edge_weights = D[0].copy()  # Distances from first node\n",
        "    parent = np.zeros(n, int)\n",
        "    edges = []\n",
        "    \n",
        "    # Use heap for more efficient minimum finding\n",
        "    import heapq\n",
        "    heap = [(edge_weights[i], 0, i) for i in range(1, n)]\n",
        "    heapq.heapify(heap)\n",
        "    \n",
        "    while heap:\n",
        "        weight, src, dst = heapq.heappop(heap)\n",
        "        if in_mst[dst]:\n",
        "            continue\n",
        "            \n",
        "        # Add edge to MST\n",
        "        edges.append((src, dst, weight))\n",
        "        in_mst[dst] = True\n",
        "        \n",
        "        # Update heap with new edges\n",
        "        for j in range(n):\n",
        "            if not in_mst[j] and D[dst, j] < edge_weights[j]:\n",
        "                edge_weights[j] = D[dst, j]\n",
        "                parent[j] = dst\n",
        "                heapq.heappush(heap, (D[dst, j], dst, j))\n",
        "    \n",
        "    # Sort edges by weight (descending)\n",
        "    edges.sort(key=lambda e: e[2], reverse=True)\n",
        "    \n",
        "    # Create adjacency list excluding k-1 heaviest edges\n",
        "    adj = [[] for _ in range(n)]\n",
        "    for u, v, w in edges[k-1:]:\n",
        "        adj[u].append(v)\n",
        "        adj[v].append(u)\n",
        "    \n",
        "    # Use efficient connected components labeling\n",
        "    labels = np.full(n, -1, dtype=int)\n",
        "    cid = 0\n",
        "    \n",
        "    for i in range(n):\n",
        "        if labels[i] >= 0:\n",
        "            continue\n",
        "            \n",
        "        # BFS for connected component\n",
        "        labels[i] = cid\n",
        "        queue = [i]\n",
        "        idx = 0\n",
        "        \n",
        "        while idx < len(queue):\n",
        "            node = queue[idx]\n",
        "            idx += 1\n",
        "            \n",
        "            for neighbor in adj[node]:\n",
        "                if labels[neighbor] < 0:\n",
        "                    labels[neighbor] = cid\n",
        "                    queue.append(neighbor)\n",
        "        \n",
        "        cid += 1\n",
        "    \n",
        "    return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqAc301OAe_P"
      },
      "source": [
        "# preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "pxk1D_KmAe_Q"
      },
      "outputs": [],
      "source": [
        "def preprocessing(X, scaling=\"robust\"):\n",
        "    if scaling == \"robust\":\n",
        "        # Handle missing values\n",
        "        missing_mask = np.isnan(X)\n",
        "        if np.any(missing_mask):\n",
        "            col_medians = np.nanmedian(X, axis=0)\n",
        "            X = np.copy(X)  # Avoid modifying original\n",
        "            for col in range(X.shape[1]):\n",
        "                X[missing_mask[:, col], col] = col_medians[col]\n",
        "        \n",
        "        # Handle outliers with vectorized operations\n",
        "        q1 = np.percentile(X, 25, axis=0)\n",
        "        q3 = np.percentile(X, 75, axis=0)\n",
        "        iqr = q3 - q1\n",
        "        lower_bounds = q1 - 1.5 * iqr  # Less aggressive outlier threshold\n",
        "        upper_bounds = q3 + 1.5 * iqr\n",
        "        \n",
        "        X_clean = np.clip(X, lower_bounds, upper_bounds)\n",
        "        \n",
        "        # Robust scaling\n",
        "        medians = np.median(X_clean, axis=0)\n",
        "        iqrs = np.percentile(X_clean, 75, axis=0) - np.percentile(X_clean, 25, axis=0)\n",
        "        \n",
        "        # Avoid division by zero\n",
        "        iqrs = np.where(iqrs > 0, iqrs, 1.0)\n",
        "        X_scaled = (X_clean - medians) / iqrs\n",
        "        \n",
        "        return X_scaled\n",
        "    \n",
        "    elif scaling == 'standard':\n",
        "        # Implement standard scaling directly\n",
        "        mean = np.mean(X, axis=0)\n",
        "        std = np.std(X, axis=0)\n",
        "        std = np.where(std > 0, std, 1.0)  # Avoid division by zero\n",
        "        return (X - mean) / std\n",
        "    \n",
        "    return X  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced feature selection with improved correlation handling\n",
        "def feature_selection(X, threshold=0.7):\n",
        "    # Calculate variances\n",
        "    variances = np.var(X, axis=0)\n",
        "    \n",
        "    # Apply variance threshold (more robust)\n",
        "    var_threshold = threshold * np.median(variances)  # Adaptive threshold\n",
        "    high_var_mask = variances > var_threshold\n",
        "    \n",
        "    if np.sum(high_var_mask) <= 1:\n",
        "        return X, high_var_mask\n",
        "    \n",
        "    X_filtered = X[:, high_var_mask]\n",
        "    original_indices = np.where(high_var_mask)[0]\n",
        "    \n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = np.corrcoef(X_filtered.T)\n",
        "    np.fill_diagonal(corr_matrix, 0)  # Zero out diagonal\n",
        "    \n",
        "    # Find highly correlated features\n",
        "    to_drop = set()\n",
        "    \n",
        "    # Sort features by variance (descending)\n",
        "    var_order = np.argsort(variances[high_var_mask])[::-1]\n",
        "    \n",
        "    # Keep higher variance features when high correlation exists\n",
        "    for i in range(len(var_order)):\n",
        "        if i in to_drop:\n",
        "            continue\n",
        "            \n",
        "        idx = var_order[i]\n",
        "        correlated = np.where(np.abs(corr_matrix[idx]) > threshold)[0]\n",
        "        \n",
        "        # Remove highly correlated features with lower variance\n",
        "        for j in correlated:\n",
        "            if j != idx and j not in to_drop:\n",
        "                to_drop.add(j)\n",
        "    \n",
        "    # Create final mask\n",
        "    keep_mask = np.ones(len(original_indices), dtype=bool)\n",
        "    keep_mask[list(to_drop)] = False\n",
        "    \n",
        "    final_indices = original_indices[keep_mask]\n",
        "    final_mask = np.zeros(X.shape[1], dtype=bool)\n",
        "    final_mask[final_indices] = True\n",
        "    \n",
        "    return X[:, final_mask], final_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPpcE9e5Ae_R"
      },
      "source": [
        "# visualize functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "g8I_eIZGAe_S"
      },
      "outputs": [],
      "source": [
        "def visualization(X_pca, labels, true_labels=None, title=\"PCA Visualization\"):\n",
        "    n_components = X_pca.shape[1]\n",
        "\n",
        "    # If we have at least 3 components, create 3D plot\n",
        "    if n_components >= 3:\n",
        "        from mpl_toolkits.mplot3d import Axes3D\n",
        "        # fig = plt.figure(figsize=(12, 10))\n",
        "        # ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "        # scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
        "        #            c=labels, cmap='viridis', marker='o', s=30, alpha=0.7)\n",
        "\n",
        "        # If true labels are available, add them to another subplot\n",
        "        if true_labels is not None:\n",
        "            # Create a side-by-side comparison\n",
        "            plt.figure(figsize=(18, 8))\n",
        "\n",
        "            # Plot clustering results\n",
        "            ax1 = plt.subplot(121, projection='3d')\n",
        "            ax1.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
        "                      c=labels, cmap='viridis', marker='o', s=30, alpha=0.7)\n",
        "            ax1.set_title(\"Clustering Results\")\n",
        "\n",
        "            # Plot true labels\n",
        "            ax2 = plt.subplot(122, projection='3d')\n",
        "            ax2.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
        "                      c=true_labels, cmap='viridis', marker='o', s=30, alpha=0.7)\n",
        "            ax2.set_title(\"True Labels\")\n",
        "\n",
        "    # Otherwise create 2D plot with the first 2 components\n",
        "    else:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', marker='o', s=30, alpha=0.7)\n",
        "\n",
        "        # Add cluster centers if we have them\n",
        "        unique_labels = np.unique(labels)\n",
        "        for label in unique_labels:\n",
        "            cluster_points = X_pca[labels == label]\n",
        "            center = np.mean(cluster_points, axis=0)\n",
        "            plt.scatter(center[0], center[1], s=200, c='red', marker='X')\n",
        "\n",
        "        # If we have true labels, create a side-by-side comparison\n",
        "        if true_labels is not None:\n",
        "            plt.figure(figsize=(18, 8))\n",
        "\n",
        "            # Plot clustering results\n",
        "            plt.subplot(121)\n",
        "            plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', marker='o', s=30, alpha=0.7)\n",
        "            plt.title(\"Clustering Results\")\n",
        "\n",
        "            # Plot true labels\n",
        "            plt.subplot(122)\n",
        "            plt.scatter(X_pca[:, 0], X_pca[:, 1], c=true_labels, cmap='viridis', marker='o', s=30, alpha=0.7)\n",
        "            plt.title(\"True Labels\")\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# hàm đánh giá phân cụm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "fhqfKICKAe_T"
      },
      "outputs": [],
      "source": [
        "def evaluate_clustering(X, labels, true_labels=None):\n",
        "    metrics_dict = {}\n",
        "    \n",
        "    # Calculate silhouette score\n",
        "    metrics_dict['silhouette'] = silhouette_score(X, labels)\n",
        "    \n",
        "    # Calculate Davies-Bouldin index\n",
        "    unique_labels = np.unique(labels)\n",
        "    n_clusters = len(unique_labels)\n",
        "    \n",
        "    if n_clusters > 1:\n",
        "        # Calculate cluster centers\n",
        "        centers = np.array([np.mean(X[labels == label], axis=0) for label in unique_labels])\n",
        "        \n",
        "        # Calculate average distances and cluster separation\n",
        "        db_values = np.zeros(n_clusters)\n",
        "        \n",
        "        for i in range(n_clusters):\n",
        "            cluster_i = X[labels == unique_labels[i]]\n",
        "            if len(cluster_i) == 0:\n",
        "                continue\n",
        "                \n",
        "            # Calculate cluster diameter (average distance to center)\n",
        "            diam_i = np.mean(np.sqrt(np.sum((cluster_i - centers[i])**2, axis=1)))\n",
        "            \n",
        "            # Find maximum ratio with other clusters\n",
        "            max_ratio = 0\n",
        "            for j in range(n_clusters):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                    \n",
        "                cluster_j = X[labels == unique_labels[j]]\n",
        "                if len(cluster_j) == 0:\n",
        "                    continue\n",
        "                    \n",
        "                diam_j = np.mean(np.sqrt(np.sum((cluster_j - centers[j])**2, axis=1)))\n",
        "                dist_ij = np.sqrt(np.sum((centers[i] - centers[j])**2))\n",
        "                \n",
        "                if dist_ij > 0:\n",
        "                    ratio = (diam_i + diam_j) / dist_ij\n",
        "                    max_ratio = max(max_ratio, ratio)\n",
        "            \n",
        "            db_values[i] = max_ratio\n",
        "        \n",
        "        metrics_dict['davies_bouldin'] = np.mean(db_values)\n",
        "    else:\n",
        "        metrics_dict['davies_bouldin'] = -1\n",
        "    \n",
        "    # Calculate external metrics if true labels provided\n",
        "    if true_labels is not None:\n",
        "        # Handle binary classification with potential label flipping\n",
        "        y_pred = np.copy(labels)\n",
        "        \n",
        "        if len(np.unique(true_labels)) == 2 and len(np.unique(y_pred)) == 2:\n",
        "            # Standardize labels to 0/1\n",
        "            y_true = (true_labels == np.max(true_labels)).astype(int)\n",
        "            y_pred = (y_pred == np.max(y_pred)).astype(int)\n",
        "            \n",
        "            # Check if labels need flipping\n",
        "            acc = accuracy_score(y_true, y_pred)\n",
        "            acc_flipped = accuracy_score(y_true, 1 - y_pred)\n",
        "            \n",
        "            if acc_flipped > acc:\n",
        "                y_pred = 1 - y_pred\n",
        "            \n",
        "            metrics_dict['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "            metrics_dict['precision'] = precision_score(y_true, y_pred)\n",
        "            metrics_dict['recall'] = recall_score(y_true, y_pred)\n",
        "            metrics_dict['f1'] = f1_score(y_true, y_pred)\n",
        "            metrics_dict['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n",
        "        else:\n",
        "            # Handle multiclass case with cluster matching\n",
        "            metrics_dict['accuracy'] = -1\n",
        "            metrics_dict['precision'] = -1\n",
        "            metrics_dict['recall'] = -1\n",
        "            metrics_dict['f1'] = -1\n",
        "            metrics_dict['confusion_matrix'] = np.zeros((2, 2))\n",
        "    \n",
        "    return metrics_dict, y_pred if true_labels is not None else labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# clustering pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "o3DG4ATOAe_T"
      },
      "outputs": [],
      "source": [
        "def clustering_pipeline(X, y_true=None, n_clusters=2, feature_selection_threshold=0.7, \n",
        "                       pca_variance_threshold=0.9, kmeans_max_iters=300, \n",
        "                       kmeans_tol=1e-6, kmeans_n_init=20, kmeans_init_method='kmeans++', \n",
        "                       kmeans_random_state=42, kmeans_early_stopping_iter=5, \n",
        "                       kmeans_batch_size=None):\n",
        "    print(\"Starting clustering pipeline...\")\n",
        "    \n",
        "    # Step 1: Preprocessing - handle outliers and scaling\n",
        "    print(\"\\nStep 1: Preprocessing...\")\n",
        "    X_preprocessed = preprocessing(X, scaling=\"robust\")  # Changed to robust scaling\n",
        "    print(f\"  Data shape after preprocessing: {X_preprocessed.shape}\")\n",
        "    \n",
        "    # Step 2: Feature selection - remove correlated and low-variance features\n",
        "    print(\"\\nStep 2: Feature selection...\")\n",
        "    X_selected, feature_mask = feature_selection(X_preprocessed, threshold=feature_selection_threshold)\n",
        "    print(f\"  Selected {X_selected.shape[1]} features out of {X_preprocessed.shape[1]}\")\n",
        "    \n",
        "    # Step 3: PCA - find optimal number of components\n",
        "    print(\"\\nStep 3: Applying PCA...\")\n",
        "    \n",
        "    # Adaptive sample size for large datasets\n",
        "    pca_sample_size = min(X_selected.shape[0], 5000)\n",
        "    if X_selected.shape[0] > pca_sample_size:\n",
        "        sample_indices = np.random.choice(X_selected.shape[0], pca_sample_size, replace=False)\n",
        "        X_pca_sample = X_selected[sample_indices]\n",
        "        pca = MyPCA()\n",
        "        pca.fit(X_pca_sample)\n",
        "    else:\n",
        "        pca = MyPCA()\n",
        "        pca.fit(X_selected)\n",
        "    \n",
        "    # Efficient component selection using explained variance\n",
        "    cumulative_var = pca.cumulative_explained_variance_\n",
        "    n_components_by_var = np.searchsorted(cumulative_var, pca_variance_threshold) + 1\n",
        "    n_components_by_var = min(n_components_by_var, X_selected.shape[1])\n",
        "    \n",
        "    # Dynamic range for silhouette test to find optimal components\n",
        "    max_components = min(50, X_selected.shape[1], n_components_by_var * 2)\n",
        "    step_size = max(1, max_components // 10)\n",
        "    component_range = list(range(2, max_components + 1, step_size))\n",
        "    \n",
        "    # Ensure we test variance threshold derived component count\n",
        "    if n_components_by_var not in component_range:\n",
        "        component_range.append(n_components_by_var)\n",
        "        component_range.sort()\n",
        "    \n",
        "    # Find optimal components by silhouette score\n",
        "    print(f\"  Testing {len(component_range)} different component counts...\")\n",
        "    best_sil = -10\n",
        "    best_n = 2\n",
        "    \n",
        "    # Try different component counts and choose best by silhouette\n",
        "    for n in component_range:\n",
        "        X_tmp = MyPCA(n_components=n).fit_transform(X_selected)\n",
        "        \n",
        "        # Quick KMeans for silhouette testing\n",
        "        labels_tmp, _, _ = kmeans(\n",
        "            X_tmp, k=n_clusters, \n",
        "            max_iters=50,  # Reduced iterations for testing\n",
        "            n_init=3,      # Fewer initializations for testing\n",
        "            random_state=kmeans_random_state\n",
        "        )\n",
        "        \n",
        "        sil_tmp = silhouette_score(X_tmp, labels_tmp)\n",
        "        acc_tmp = accuracy_score(y_true, labels_tmp)\n",
        "        print(f\"    Components: {n}, Silhouette: {sil_tmp:.4f}, Accuracy: {acc_tmp:.4f}\")\n",
        "        \n",
        "        if sil_tmp > best_sil:\n",
        "            best_sil, best_n = sil_tmp, n\n",
        "    \n",
        "    n_components = min(best_n, int(0.95 * X_selected.shape[1]))\n",
        "    print(f\"  Optimal PCA components: {n_components} (silhouette={best_sil:.4f})\")\n",
        "    \n",
        "    # Final PCA transformation\n",
        "    pca = MyPCA(n_components=n_components)\n",
        "    X_pca = pca.fit_transform(X_selected)\n",
        "    \n",
        "    # Step 4: Run multiple clustering algorithms\n",
        "    print(\"\\nStep 4: Comparing clustering methods...\")\n",
        "    methods = {}\n",
        "    \n",
        "    # K-means clustering\n",
        "    labels_k, centers_k, inertia_k = kmeans(\n",
        "        X_pca, k=n_clusters,\n",
        "        max_iters=kmeans_max_iters, \n",
        "        tol=kmeans_tol,\n",
        "        n_init=kmeans_n_init, \n",
        "        init_method=kmeans_init_method,\n",
        "        random_state=kmeans_random_state,\n",
        "        early_stopping_iter=kmeans_early_stopping_iter,\n",
        "        batch_size=kmeans_batch_size\n",
        "    )\n",
        "    sil_k = silhouette_score(X_pca, labels_k)\n",
        "    print(f\"  KMeans silhouette = {sil_k:.4f}\")\n",
        "    methods['kmeans'] = (labels_k, sil_k)\n",
        "    \n",
        "    # Agglomerative clustering\n",
        "    labels_agg = agglomerative_mst(X_pca, k=n_clusters)\n",
        "    sil_agg = silhouette_score(X_pca, labels_agg)\n",
        "    print(f\"  Agglomerative-MST silhouette = {sil_agg:.4f}\")\n",
        "    methods['agglomerative'] = (labels_agg, sil_agg)\n",
        "    \n",
        "    # Try a hybrid approach - initialize K-means with agglomerative results\n",
        "    centers_hybrid = np.array([np.mean(X_pca[labels_agg == i], axis=0) \n",
        "                              for i in range(n_clusters)])\n",
        "    labels_hybrid, _, _ = kmeans(\n",
        "        X_pca, k=n_clusters,\n",
        "        max_iters=kmeans_max_iters,\n",
        "        tol=kmeans_tol,\n",
        "        n_init=1,  # Only one initialization with agglomerative centers\n",
        "        init_method='custom',\n",
        "        random_state=kmeans_random_state\n",
        "    )\n",
        "    sil_hybrid = silhouette_score(X_pca, labels_hybrid)\n",
        "    print(f\"  Hybrid (Agg+KMeans) silhouette = {sil_hybrid:.4f}\")\n",
        "    methods['hybrid'] = (labels_hybrid, sil_hybrid)\n",
        "    \n",
        "    # Select best method\n",
        "    best_method = max(methods, key=lambda m: methods[m][1])\n",
        "    best_labels = methods[best_method][0]\n",
        "    best_sil = methods[best_method][1]\n",
        "    print(f\"  => Selected {best_method} with silhouette = {best_sil:.4f}\")\n",
        "    \n",
        "    # Evaluate final clustering\n",
        "    best_metrics, best_labels = evaluate_clustering(X_pca, best_labels, y_true)\n",
        "    \n",
        "    # Print final metrics\n",
        "    print(\"\\nFinal Results:\")\n",
        "    print(f\"Algorithm: {best_method}\")\n",
        "    print(f\"Number of PCA components: {n_components}\")\n",
        "    for metric, value in best_metrics.items():\n",
        "        if metric != 'confusion_matrix':\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "    \n",
        "    if y_true is not None:\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(best_metrics['confusion_matrix'])\n",
        "    \n",
        "    return best_method, best_labels, best_metrics, n_components, X_pca, feature_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgiwK-aoAe_U"
      },
      "source": [
        "# Chuẩn bị dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "32hoPCpvAe_U"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../data/ABIDE2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "MuyAYxtPAe_V",
        "outputId": "bd3a6ccd-53a1-44cb-ea57-ce06240a768b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kích thước dataset: (1004, 1445)\n",
            "Số lượng mỗi nhóm:\n",
            "group\n",
            "Normal    541\n",
            "Cancer    463\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(f\"Kích thước dataset: {df.shape}\")\n",
        "print(f\"Số lượng mỗi nhóm:\")\n",
        "print(df['group'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "e1M6hAbIAe_W"
      },
      "outputs": [],
      "source": [
        "# Assume these columns are not used for clustering\n",
        "X = df.drop(columns=['site', 'subject', 'group']).values\n",
        "y = df['group'].values\n",
        "\n",
        "# Convert labels to numeric (0: Normal, 1: Cancer)\n",
        "y_numeric = np.where(y == 'Cancer', 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcHjGJgmAe_W"
      },
      "source": [
        "# Tìm các tham số tối ưu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "ZCbBTnEHAe_W",
        "outputId": "9e1c3b84-56f2-447f-e4e0-dfa2ca89fe71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting clustering pipeline...\n",
            "\n",
            "Step 1: Preprocessing...\n",
            "  Data shape after preprocessing: (1004, 1442)\n",
            "\n",
            "Step 2: Feature selection...\n",
            "  Selected 419 features out of 1442\n",
            "\n",
            "Step 3: Applying PCA...\n",
            "  Testing 11 different component counts...\n",
            "    Components: 2, Silhouette: 0.6862, Accuracy: 0.4731\n",
            "    Components: 7, Silhouette: 0.5785, Accuracy: 0.5269\n",
            "    Components: 12, Silhouette: 0.5455, Accuracy: 0.4731\n",
            "    Components: 17, Silhouette: 0.5228, Accuracy: 0.5269\n",
            "    Components: 22, Silhouette: 0.5057, Accuracy: 0.5269\n",
            "    Components: 27, Silhouette: 0.4914, Accuracy: 0.5269\n",
            "    Components: 32, Silhouette: 0.4793, Accuracy: 0.5269\n",
            "    Components: 37, Silhouette: 0.4688, Accuracy: 0.5269\n",
            "    Components: 42, Silhouette: 0.4597, Accuracy: 0.5269\n",
            "    Components: 47, Silhouette: 0.4515, Accuracy: 0.5269\n",
            "    Components: 183, Silhouette: 0.3628, Accuracy: 0.5279\n",
            "  Optimal PCA components: 2 (silhouette=0.6862)\n",
            "\n",
            "Step 4: Comparing clustering methods...\n",
            "  KMeans silhouette = 0.6862\n",
            "  Agglomerative-MST silhouette = 0.4403\n",
            "  Hybrid (Agg+KMeans) silhouette = 0.6862\n",
            "  => Selected kmeans with silhouette = 0.6862\n",
            "\n",
            "Final Results:\n",
            "Algorithm: kmeans\n",
            "Number of PCA components: 2\n",
            "silhouette: 0.6862\n",
            "davies_bouldin: 0.7538\n",
            "accuracy: 0.5269\n",
            "precision: 0.4830\n",
            "recall: 0.3672\n",
            "f1: 0.4172\n",
            "Confusion Matrix:\n",
            "[[359 182]\n",
            " [293 170]]\n"
          ]
        }
      ],
      "source": [
        "# Run the improved clustering pipeline\n",
        "final_method, final_labels, final_metrics, n_components, X_pca, feature_mask = clustering_pipeline(X, y_numeric, \n",
        "                                                                                feature_selection_threshold=0.7, \n",
        "                                                                                pca_variance_threshold=0.9, \n",
        "                                                                                kmeans_max_iters=300, kmeans_n_init=20\n",
        "                                                                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbfxRi59Ae_W"
      },
      "source": [
        "# Kết quả"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "FQmiNApKAe_W",
        "outputId": "0203d02b-703f-4aa1-c9de-5ffeae51aef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Results:\n",
            "Thuật toán: kmeans\n",
            "Number of PCA components: 2\n",
            "Accuracy: 0.5269\n",
            "Precision: 0.4830\n",
            "Recall: 0.3672\n",
            "F1 score: 0.4172\n",
            "Silhouette score: 0.6862\n"
          ]
        }
      ],
      "source": [
        "# Print final results\n",
        "print(\"Final Results:\")\n",
        "print(f'Thuật toán: {final_method}')\n",
        "print(f\"Number of PCA components: {n_components}\")\n",
        "print(f\"Accuracy: {final_metrics['accuracy']:.4f}\")\n",
        "print(f\"Precision: {final_metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {final_metrics['recall']:.4f}\")\n",
        "print(f\"F1 score: {final_metrics['f1']:.4f}\")\n",
        "print(f\"Silhouette score: {final_metrics['silhouette']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# So sánh với GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GMM metrics:\n",
            "silhouette: 0.6890864537083208\n",
            "davies_bouldin: 0.741774962593511\n",
            "accuracy: 0.5398406374501992\n",
            "precision: 0.5015673981191222\n",
            "recall: 0.34557235421166305\n",
            "f1: 0.40920716112531963\n",
            "Confusion Matrix:\n",
            "[[382 159]\n",
            " [303 160]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gmm = GaussianMixture(n_components=2)\n",
        "labels = gmm.fit_predict(X_pca)\n",
        "metrics, remapped_labels = evaluate_clustering(X_pca, labels, y_numeric)\n",
        "print(\"GMM metrics:\")\n",
        "for metric, value in metrics.items():\n",
        "    if metric != 'confusion_matrix':\n",
        "        print(f\"{metric}: {value}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(metrics['confusion_matrix'])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
